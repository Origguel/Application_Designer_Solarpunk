# app/utils/keyword_extractor.py

import json
from pathlib import Path
import threading

# Pas de spacy import√© au d√©but
nlp = None
nlp_ready = False
nlp_lock = threading.Lock()

TERMS_PATH = Path("assets/keyword/termes_interdits.json")
if TERMS_PATH.exists():
    with TERMS_PATH.open(encoding="utf-8") as f:
        termes_interdits = set(json.load(f))
else:
    termes_interdits = set()

def _load_nlp_model():
    """Charge spaCy dans un thread."""
    global nlp, nlp_ready
    import spacy  # üëà Import spacy ici seulement
    nlp = spacy.load("fr_core_news_sm")
    with nlp_lock:
        nlp_ready = True
    print("‚úÖ Mod√®le spaCy charg√©.")

def ensure_nlp_ready():
    """Assure que le mod√®le est charg√© (sinon lance le thread)."""
    global nlp_ready

    if not nlp_ready:
        print("‚ö° Chargement du mod√®le spaCy d√©clench√©...")
        threading.Thread(target=_load_nlp_model, daemon=True).start()

def extract_keywords(text):
    ensure_nlp_ready()

    global nlp

    if not nlp_ready:
        print("‚è≥ Mod√®le spaCy pas encore pr√™t, petite attente...")
        import time
        timeout = 5
        start = time.time()
        while not nlp_ready and (time.time() - start) < timeout:
            time.sleep(0.1)

        if not nlp_ready:
            print("‚ùó Mod√®le non charg√© apr√®s 5s, extraction annul√©e.")
            return []

    doc = nlp(text.lower())
    candidates = {}

    for token in doc:
        mot = token.lemma_.lower()
        pos = token.pos_

        # ‚ùå Filtres de base
        if token.is_stop or token.is_punct or token.like_num:
            continue
        if len(mot) < 3 or mot in termes_interdits:
            continue
        if not mot.isalpha():
            continue
        if sum(1 for c in mot if c in "aeiouy") < 2:
            continue

        # ‚úÖ POS cibl√©s
        if pos in {"NOUN", "PROPN", "ADJ"}:
            score = 1
            if pos == "PROPN":
                score += 2
            elif pos == "ADJ":
                score += 0.5
            if mot in candidates:
                candidates[mot] += score
            else:
                candidates[mot] = score

    # üîΩ Tri par pertinence, puis unicit√©
    sorted_keywords = sorted(candidates.items(), key=lambda x: -x[1])
    mots_uniques = []
    seen = set()
    for mot, _ in sorted_keywords:
        if mot not in seen:
            seen.add(mot)
            mots_uniques.append(mot)

    return mots_uniques[:6]
